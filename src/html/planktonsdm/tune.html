<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>planktonsdm.tune API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>planktonsdm.tune</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
os.environ[&#39;MKL_NUM_THREADS&#39;] = &#39;1&#39;
os.environ[&#39;OMP_NUM_THREADS&#39;] = &#39;1&#39;
os.environ[&#39;MKL_DYNAMIC&#39;] = &#39;FALSE&#39;
os.environ[&#34;OMP_THREAD_LIMIT&#34;] = &#34;1&#34;

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
import pickle
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import OneHotEncoder
from joblib import parallel_backend
import time
import numpy as np
from sklearn.model_selection import StratifiedKFold
from scipy.stats import kendalltau
from sklearn.compose import TransformedTargetRegressor
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.ensemble import BaggingRegressor, BaggingClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.base import BaseEstimator, RegressorMixin, clone, is_regressor, is_classifier
from sklearn.utils.validation import check_is_fitted, check_X_y, check_array
from sklearn.exceptions import NotFittedError
from inspect import signature
import logging


from planktonsdm.functions import do_log, do_exp,  ZeroInflatedRegressor, LogGridSearch, ZeroStratifiedKFold, UpsampledZeroStratifiedKFold, tau_scoring, tau_scoring_p

class tune:

    def __init__(self, X, y, model_config, scale=True):

        &#34;&#34;&#34;
        Tuning function

        Parameters
        ----------

        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Internally, its dtype will be converted
            to ``dtype=np.float32``. If a sparse matrix is provided, it will be
            converted into a sparse ``csc_matrix``.

        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
            The target values (class labels in classification, real numbers in
            regression).

        model_config: dictionary, default=None
            A dictionary containing:

            `seed` : int, used to create random numbers
            
            `root`: string, path to folder
            
            `path_out`: string, where predictions are saved
            
            `path_in`: string, where to find tuned models
            
            `traits`: string, file name of your trait file
            
            `verbose`: int, to set verbosity (0-3)
            
            `n_threads`: int, number of threads to use
            
            `cv` : int, number of cross-folds
                        
            `ensemble_config` : 
            
            `clf_scoring` :
            
            `reg_scoring` :

            
        scale : bool, default=False
            If True, normalize X before training

        &#34;&#34;&#34;


        self.y = y.sample(frac=1, random_state=model_config[&#39;seed&#39;]) #shuffle
        if scale==True:
            scaler = StandardScaler()  
            scaler.fit(X)  
            self.X = pd.DataFrame(scaler.transform(X))
            print(&#34;scale X = True&#34;)

        else:
            self.X = X
        self.X = self.X.sample(frac=1, random_state=model_config[&#39;seed&#39;]) #shuffle
        self.model_config = model_config
        self.seed = model_config[&#39;seed&#39;]
        self.species = y.name
        self.n_jobs = model_config[&#39;n_threads&#39;]
        self.verbose = model_config[&#39;verbose&#39;] 
        self.path_out =  model_config[&#39;root&#39;] + model_config[&#39;path_out&#39;]

        if model_config[&#39;upsample&#39;]==True:
            self.cv = UpsampledZeroStratifiedKFold(n_splits=model_config[&#39;cv&#39;])
            print(&#34;upsampling = True&#34;)
        else:
            self.cv = ZeroStratifiedKFold(n_splits=model_config[&#39;cv&#39;])
            
        try:
            self.bagging_estimators = model_config[&#39;knn_bagging_estimators&#39;] 
        except:
            self.bagging_estimators = None

    
    def train(self, model, classifier=False, regressor=False, log=&#34;no&#34;):

        &#34;&#34;&#34;
        Training function

        Parameters
        ----------
        model : string, default=&#34;rf&#34;
            Which model to train: 
            Supported models:
            `&#34;rf&#34;` Random Forest 
            `&#34;knn&#34;` K-Nearest Neighbors
            `&#34;xgb&#34;` XGBoost

        classifier : bool, default=False

        regressor : bool, default=False

        log : string, default=&#34;no&#34;
            If `&#34;yes&#34;`, log transformation is applied to y
            
            If `&#34;no&#34;`, y is not transformed
            
            If `&#34;both&#34;`, both log and no-log transformations are fitted by 
                running the model two times.

        Notes
        -----
        Requires `tune` to be initialized
        &#34;&#34;&#34;

        if model ==&#34;xgb&#34;:
            clf_estimator = XGBClassifier(nthread=1)
            reg_estimator = XGBRegressor(nthread=1)
        elif model==&#34;knn&#34;:
            if self.bagging_estimators ==None:
                print(&#34;you forgot to define the number of bagging estimators&#34;)
            else:
                clf_estimator = BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=self.bagging_estimators)
                reg_estimator = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=self.bagging_estimators)
        elif model==&#34;rf&#34;:
            clf_estimator = RandomForestClassifier(random_state=self.seed, oob_score=True)
            reg_estimator = RandomForestRegressor(random_state=self.seed, oob_score=True)
        else:
            print(&#34;invalid model&#34;)


        if classifier == False and regressor ==False:
            print(&#34;both classifier defined as False&#34;)
            print(&#34;no model was trained&#34;)

        if classifier ==True:
            print(&#34;training classifier&#34;)
            clf_param_grid = self.model_config[&#39;param_grid&#39;][model + &#39;_param_grid&#39;][&#39;clf_param_grid&#39;]
            clf_scoring = self.model_config[&#39;clf_scoring&#39;]

            clf_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
            clf_sav_out_model = self.path_out + model + &#34;/model/&#34;


            try: #make new dir if needed
                os.makedirs(clf_sav_out_scores)
            except:
                None


            try: #make new dir if needed
                os.makedirs(clf_sav_out_model)
            except:
                None



            clf = GridSearchCV(
                estimator=clf_estimator,
                param_grid= clf_param_grid,
                scoring= &#39;balanced_accuracy&#39;,
                cv = self.cv,
                verbose = self.verbose
            )

            y_clf =  self.y.copy()
            y_clf[y_clf &gt; 0] = 1

            with parallel_backend(&#39;multiprocessing&#39;, self.n_jobs):
                clf.fit(self.X, y_clf.values.ravel())

            m1 = clf.best_estimator_
            pickle.dump(m1, open(clf_sav_out_model + self.species + &#39;_clf.sav&#39;, &#39;wb&#39;))
            print(&#34;exported model to:&#34; + clf_sav_out_model + self.species + &#39;_clf.sav&#39;)


            clf_scores = cross_validate(m1, self.X, y_clf.values.ravel(), cv=self.cv, verbose =self.verbose, scoring=clf_scoring)
            pickle.dump(clf_scores, open(clf_sav_out_scores + self.species + &#39;_clf.sav&#39;, &#39;wb&#39;))
            print(&#34;exported scoring to: &#34; + clf_sav_out_scores + self.species + &#39;_clf.sav&#39;)

            print(clf_scores[&#39;test_accuracy&#39;])
            print(&#34;clf balanced accuracy &#34; + str((round(np.mean(clf_scores[&#39;test_accuracy&#39;]), 2))))




        if regressor ==True:
            print(&#34;training regressor&#34;)

            reg_scoring = self.model_config[&#39;reg_scoring&#39;]
            reg_param_grid = self.model_config[&#39;param_grid&#39;][model + &#39;_param_grid&#39;][&#39;reg_param_grid&#39;]



            reg_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
            reg_sav_out_model = self.path_out + model + &#34;/model/&#34;



            try: #make new dir if needed
                os.makedirs(reg_sav_out_scores)
            except:
                None

            try: #make new dir if needed
                os.makedirs(reg_sav_out_model)
            except:
                None


            with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
                reg = LogGridSearch(reg_estimator, verbose = self.verbose, cv=self.cv, param_grid=reg_param_grid, scoring=&#34;neg_mean_absolute_error&#34;)
                reg_grid_search = reg.transformed_fit(self.X, self.y.values.ravel(), log)

            m2 = reg_grid_search.best_estimator_
            pickle.dump(m2, open(reg_sav_out_model  + self.species + &#39;_reg.sav&#39;, &#39;wb&#39;))

            print(&#34;exported model to: &#34; + reg_sav_out_model  + self.species + &#39;_reg.sav&#39;)

            with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
                reg_scores = cross_validate(m2, self.X, self.y.values.ravel(), cv = self.cv, verbose = self.verbose, scoring=reg_scoring)

            pickle.dump(reg_scores, open(reg_sav_out_scores + self.species + &#39;_reg.sav&#39;, &#39;wb&#39;))


            print(&#34;exported scoring to: &#34; + reg_sav_out_scores + self.species + &#39;_reg.sav&#39;)

            print(&#34;reg rRMSE: &#34; + str(int(round(np.mean(reg_scores[&#39;test_RMSE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;reg rMAE: &#34; + str(int(round(np.mean(reg_scores[&#39;test_MAE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;reg R2: &#34; + str(round(np.mean(reg_scores[&#39;test_R2&#39;]), 2)))



        if (classifier ==True) and (regressor ==True):
            print(&#34;training zero-inflated regressor&#34;)

            zir = ZeroInflatedRegressor(
                classifier=m1,
                regressor=m2,
            )

            zir_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
            zir_sav_out_model = self.path_out + model + &#34;/model/&#34;

            try: #make new dir if needed
                os.makedirs(zir_sav_out_scores)
            except:
                None

            try: #make new dir if needed
                os.makedirs(zir_sav_out_model)
            except:
                None           

            pickle.dump(zir, open(zir_sav_out_model + self.species + &#39;_zir.sav&#39;, &#39;wb&#39;))
            print(&#34;exported model to: &#34; + zir_sav_out_model + self.species + &#39;_zir.sav&#39;)


            with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
                zir_scores = cross_validate(zir, self.X, self.y.ravel(), cv=self.cv, verbose =self.verbose, scoring=reg_scoring)


            pickle.dump(zir_scores, open(zir_sav_out_scores + self.species + &#39;_zir.sav&#39;, &#39;wb&#39;))
            print(&#34;exported scoring to: &#34; + zir_sav_out_scores + self.species + &#39;_zir.sav&#39;)

            print(&#34;zir rRMSE: &#34; + str(int(round(np.mean(zir_scores[&#39;test_RMSE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;zir rMAE: &#34; + str(int(round(np.mean(zir_scores[&#39;test_MAE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;zir R2: &#34; + str(round(np.mean(zir_scores[&#39;test_R2&#39;]), 2)))



        st = time.time()
        et = time.time()
        elapsed_time = et-st

        print(&#34;execution time:&#34;, elapsed_time, &#34;seconds&#34;)        </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="planktonsdm.tune.tune"><code class="flex name class">
<span>class <span class="ident">tune</span></span>
<span>(</span><span>X, y, model_config, scale=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Tuning function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The training input samples. Internally, its dtype will be converted
to <code>dtype=np.float32</code>. If a sparse matrix is provided, it will be
converted into a sparse <code>csc_matrix</code>.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code> or <code>(n_samples, n_outputs)</code></dt>
<dd>The target values (class labels in classification, real numbers in
regression).</dd>
<dt><strong><code>model_config</code></strong> :&ensp;<code>dictionary</code>, default=<code>None</code></dt>
<dd>
<p>A dictionary containing:</p>
<p><code>seed</code> : int, used to create random numbers</p>
<p><code>root</code>: string, path to folder</p>
<p><code>path_out</code>: string, where predictions are saved</p>
<p><code>path_in</code>: string, where to find tuned models</p>
<p><code>traits</code>: string, file name of your trait file</p>
<p><code>verbose</code>: int, to set verbosity (0-3)</p>
<p><code>n_threads</code>: int, number of threads to use</p>
<p><code>cv</code> : int, number of cross-folds</p>
<p><code>ensemble_config</code> : </p>
<p><code>clf_scoring</code> :</p>
<p><code>reg_scoring</code> :</p>
</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If True, normalize X before training</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class tune:

    def __init__(self, X, y, model_config, scale=True):

        &#34;&#34;&#34;
        Tuning function

        Parameters
        ----------

        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The training input samples. Internally, its dtype will be converted
            to ``dtype=np.float32``. If a sparse matrix is provided, it will be
            converted into a sparse ``csc_matrix``.

        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
            The target values (class labels in classification, real numbers in
            regression).

        model_config: dictionary, default=None
            A dictionary containing:

            `seed` : int, used to create random numbers
            
            `root`: string, path to folder
            
            `path_out`: string, where predictions are saved
            
            `path_in`: string, where to find tuned models
            
            `traits`: string, file name of your trait file
            
            `verbose`: int, to set verbosity (0-3)
            
            `n_threads`: int, number of threads to use
            
            `cv` : int, number of cross-folds
                        
            `ensemble_config` : 
            
            `clf_scoring` :
            
            `reg_scoring` :

            
        scale : bool, default=False
            If True, normalize X before training

        &#34;&#34;&#34;


        self.y = y.sample(frac=1, random_state=model_config[&#39;seed&#39;]) #shuffle
        if scale==True:
            scaler = StandardScaler()  
            scaler.fit(X)  
            self.X = pd.DataFrame(scaler.transform(X))
            print(&#34;scale X = True&#34;)

        else:
            self.X = X
        self.X = self.X.sample(frac=1, random_state=model_config[&#39;seed&#39;]) #shuffle
        self.model_config = model_config
        self.seed = model_config[&#39;seed&#39;]
        self.species = y.name
        self.n_jobs = model_config[&#39;n_threads&#39;]
        self.verbose = model_config[&#39;verbose&#39;] 
        self.path_out =  model_config[&#39;root&#39;] + model_config[&#39;path_out&#39;]

        if model_config[&#39;upsample&#39;]==True:
            self.cv = UpsampledZeroStratifiedKFold(n_splits=model_config[&#39;cv&#39;])
            print(&#34;upsampling = True&#34;)
        else:
            self.cv = ZeroStratifiedKFold(n_splits=model_config[&#39;cv&#39;])
            
        try:
            self.bagging_estimators = model_config[&#39;knn_bagging_estimators&#39;] 
        except:
            self.bagging_estimators = None

    
    def train(self, model, classifier=False, regressor=False, log=&#34;no&#34;):

        &#34;&#34;&#34;
        Training function

        Parameters
        ----------
        model : string, default=&#34;rf&#34;
            Which model to train: 
            Supported models:
            `&#34;rf&#34;` Random Forest 
            `&#34;knn&#34;` K-Nearest Neighbors
            `&#34;xgb&#34;` XGBoost

        classifier : bool, default=False

        regressor : bool, default=False

        log : string, default=&#34;no&#34;
            If `&#34;yes&#34;`, log transformation is applied to y
            
            If `&#34;no&#34;`, y is not transformed
            
            If `&#34;both&#34;`, both log and no-log transformations are fitted by 
                running the model two times.

        Notes
        -----
        Requires `tune` to be initialized
        &#34;&#34;&#34;

        if model ==&#34;xgb&#34;:
            clf_estimator = XGBClassifier(nthread=1)
            reg_estimator = XGBRegressor(nthread=1)
        elif model==&#34;knn&#34;:
            if self.bagging_estimators ==None:
                print(&#34;you forgot to define the number of bagging estimators&#34;)
            else:
                clf_estimator = BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=self.bagging_estimators)
                reg_estimator = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=self.bagging_estimators)
        elif model==&#34;rf&#34;:
            clf_estimator = RandomForestClassifier(random_state=self.seed, oob_score=True)
            reg_estimator = RandomForestRegressor(random_state=self.seed, oob_score=True)
        else:
            print(&#34;invalid model&#34;)


        if classifier == False and regressor ==False:
            print(&#34;both classifier defined as False&#34;)
            print(&#34;no model was trained&#34;)

        if classifier ==True:
            print(&#34;training classifier&#34;)
            clf_param_grid = self.model_config[&#39;param_grid&#39;][model + &#39;_param_grid&#39;][&#39;clf_param_grid&#39;]
            clf_scoring = self.model_config[&#39;clf_scoring&#39;]

            clf_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
            clf_sav_out_model = self.path_out + model + &#34;/model/&#34;


            try: #make new dir if needed
                os.makedirs(clf_sav_out_scores)
            except:
                None


            try: #make new dir if needed
                os.makedirs(clf_sav_out_model)
            except:
                None



            clf = GridSearchCV(
                estimator=clf_estimator,
                param_grid= clf_param_grid,
                scoring= &#39;balanced_accuracy&#39;,
                cv = self.cv,
                verbose = self.verbose
            )

            y_clf =  self.y.copy()
            y_clf[y_clf &gt; 0] = 1

            with parallel_backend(&#39;multiprocessing&#39;, self.n_jobs):
                clf.fit(self.X, y_clf.values.ravel())

            m1 = clf.best_estimator_
            pickle.dump(m1, open(clf_sav_out_model + self.species + &#39;_clf.sav&#39;, &#39;wb&#39;))
            print(&#34;exported model to:&#34; + clf_sav_out_model + self.species + &#39;_clf.sav&#39;)


            clf_scores = cross_validate(m1, self.X, y_clf.values.ravel(), cv=self.cv, verbose =self.verbose, scoring=clf_scoring)
            pickle.dump(clf_scores, open(clf_sav_out_scores + self.species + &#39;_clf.sav&#39;, &#39;wb&#39;))
            print(&#34;exported scoring to: &#34; + clf_sav_out_scores + self.species + &#39;_clf.sav&#39;)

            print(clf_scores[&#39;test_accuracy&#39;])
            print(&#34;clf balanced accuracy &#34; + str((round(np.mean(clf_scores[&#39;test_accuracy&#39;]), 2))))




        if regressor ==True:
            print(&#34;training regressor&#34;)

            reg_scoring = self.model_config[&#39;reg_scoring&#39;]
            reg_param_grid = self.model_config[&#39;param_grid&#39;][model + &#39;_param_grid&#39;][&#39;reg_param_grid&#39;]



            reg_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
            reg_sav_out_model = self.path_out + model + &#34;/model/&#34;



            try: #make new dir if needed
                os.makedirs(reg_sav_out_scores)
            except:
                None

            try: #make new dir if needed
                os.makedirs(reg_sav_out_model)
            except:
                None


            with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
                reg = LogGridSearch(reg_estimator, verbose = self.verbose, cv=self.cv, param_grid=reg_param_grid, scoring=&#34;neg_mean_absolute_error&#34;)
                reg_grid_search = reg.transformed_fit(self.X, self.y.values.ravel(), log)

            m2 = reg_grid_search.best_estimator_
            pickle.dump(m2, open(reg_sav_out_model  + self.species + &#39;_reg.sav&#39;, &#39;wb&#39;))

            print(&#34;exported model to: &#34; + reg_sav_out_model  + self.species + &#39;_reg.sav&#39;)

            with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
                reg_scores = cross_validate(m2, self.X, self.y.values.ravel(), cv = self.cv, verbose = self.verbose, scoring=reg_scoring)

            pickle.dump(reg_scores, open(reg_sav_out_scores + self.species + &#39;_reg.sav&#39;, &#39;wb&#39;))


            print(&#34;exported scoring to: &#34; + reg_sav_out_scores + self.species + &#39;_reg.sav&#39;)

            print(&#34;reg rRMSE: &#34; + str(int(round(np.mean(reg_scores[&#39;test_RMSE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;reg rMAE: &#34; + str(int(round(np.mean(reg_scores[&#39;test_MAE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;reg R2: &#34; + str(round(np.mean(reg_scores[&#39;test_R2&#39;]), 2)))



        if (classifier ==True) and (regressor ==True):
            print(&#34;training zero-inflated regressor&#34;)

            zir = ZeroInflatedRegressor(
                classifier=m1,
                regressor=m2,
            )

            zir_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
            zir_sav_out_model = self.path_out + model + &#34;/model/&#34;

            try: #make new dir if needed
                os.makedirs(zir_sav_out_scores)
            except:
                None

            try: #make new dir if needed
                os.makedirs(zir_sav_out_model)
            except:
                None           

            pickle.dump(zir, open(zir_sav_out_model + self.species + &#39;_zir.sav&#39;, &#39;wb&#39;))
            print(&#34;exported model to: &#34; + zir_sav_out_model + self.species + &#39;_zir.sav&#39;)


            with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
                zir_scores = cross_validate(zir, self.X, self.y.ravel(), cv=self.cv, verbose =self.verbose, scoring=reg_scoring)


            pickle.dump(zir_scores, open(zir_sav_out_scores + self.species + &#39;_zir.sav&#39;, &#39;wb&#39;))
            print(&#34;exported scoring to: &#34; + zir_sav_out_scores + self.species + &#39;_zir.sav&#39;)

            print(&#34;zir rRMSE: &#34; + str(int(round(np.mean(zir_scores[&#39;test_RMSE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;zir rMAE: &#34; + str(int(round(np.mean(zir_scores[&#39;test_MAE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
            print(&#34;zir R2: &#34; + str(round(np.mean(zir_scores[&#39;test_R2&#39;]), 2)))



        st = time.time()
        et = time.time()
        elapsed_time = et-st

        print(&#34;execution time:&#34;, elapsed_time, &#34;seconds&#34;)        </code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="planktonsdm.tune.tune.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, model, classifier=False, regressor=False, log='no')</span>
</code></dt>
<dd>
<div class="desc"><p>Training function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>string</code>, default=<code>"rf"</code></dt>
<dd>Which model to train:
Supported models:
<code>"rf"</code> Random Forest
<code>"knn"</code> K-Nearest Neighbors
<code>"xgb"</code> XGBoost</dd>
<dt><strong><code>classifier</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>regressor</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>string</code>, default=<code>"no"</code></dt>
<dd>
<p>If <code>"yes"</code>, log transformation is applied to y</p>
<p>If <code>"no"</code>, y is not transformed</p>
<p>If <code>"both"</code>, both log and no-log transformations are fitted by
running the model two times.</p>
</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Requires <code><a title="planktonsdm.tune.tune" href="#planktonsdm.tune.tune">tune</a></code> to be initialized</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, model, classifier=False, regressor=False, log=&#34;no&#34;):

    &#34;&#34;&#34;
    Training function

    Parameters
    ----------
    model : string, default=&#34;rf&#34;
        Which model to train: 
        Supported models:
        `&#34;rf&#34;` Random Forest 
        `&#34;knn&#34;` K-Nearest Neighbors
        `&#34;xgb&#34;` XGBoost

    classifier : bool, default=False

    regressor : bool, default=False

    log : string, default=&#34;no&#34;
        If `&#34;yes&#34;`, log transformation is applied to y
        
        If `&#34;no&#34;`, y is not transformed
        
        If `&#34;both&#34;`, both log and no-log transformations are fitted by 
            running the model two times.

    Notes
    -----
    Requires `tune` to be initialized
    &#34;&#34;&#34;

    if model ==&#34;xgb&#34;:
        clf_estimator = XGBClassifier(nthread=1)
        reg_estimator = XGBRegressor(nthread=1)
    elif model==&#34;knn&#34;:
        if self.bagging_estimators ==None:
            print(&#34;you forgot to define the number of bagging estimators&#34;)
        else:
            clf_estimator = BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=self.bagging_estimators)
            reg_estimator = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=self.bagging_estimators)
    elif model==&#34;rf&#34;:
        clf_estimator = RandomForestClassifier(random_state=self.seed, oob_score=True)
        reg_estimator = RandomForestRegressor(random_state=self.seed, oob_score=True)
    else:
        print(&#34;invalid model&#34;)


    if classifier == False and regressor ==False:
        print(&#34;both classifier defined as False&#34;)
        print(&#34;no model was trained&#34;)

    if classifier ==True:
        print(&#34;training classifier&#34;)
        clf_param_grid = self.model_config[&#39;param_grid&#39;][model + &#39;_param_grid&#39;][&#39;clf_param_grid&#39;]
        clf_scoring = self.model_config[&#39;clf_scoring&#39;]

        clf_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
        clf_sav_out_model = self.path_out + model + &#34;/model/&#34;


        try: #make new dir if needed
            os.makedirs(clf_sav_out_scores)
        except:
            None


        try: #make new dir if needed
            os.makedirs(clf_sav_out_model)
        except:
            None



        clf = GridSearchCV(
            estimator=clf_estimator,
            param_grid= clf_param_grid,
            scoring= &#39;balanced_accuracy&#39;,
            cv = self.cv,
            verbose = self.verbose
        )

        y_clf =  self.y.copy()
        y_clf[y_clf &gt; 0] = 1

        with parallel_backend(&#39;multiprocessing&#39;, self.n_jobs):
            clf.fit(self.X, y_clf.values.ravel())

        m1 = clf.best_estimator_
        pickle.dump(m1, open(clf_sav_out_model + self.species + &#39;_clf.sav&#39;, &#39;wb&#39;))
        print(&#34;exported model to:&#34; + clf_sav_out_model + self.species + &#39;_clf.sav&#39;)


        clf_scores = cross_validate(m1, self.X, y_clf.values.ravel(), cv=self.cv, verbose =self.verbose, scoring=clf_scoring)
        pickle.dump(clf_scores, open(clf_sav_out_scores + self.species + &#39;_clf.sav&#39;, &#39;wb&#39;))
        print(&#34;exported scoring to: &#34; + clf_sav_out_scores + self.species + &#39;_clf.sav&#39;)

        print(clf_scores[&#39;test_accuracy&#39;])
        print(&#34;clf balanced accuracy &#34; + str((round(np.mean(clf_scores[&#39;test_accuracy&#39;]), 2))))




    if regressor ==True:
        print(&#34;training regressor&#34;)

        reg_scoring = self.model_config[&#39;reg_scoring&#39;]
        reg_param_grid = self.model_config[&#39;param_grid&#39;][model + &#39;_param_grid&#39;][&#39;reg_param_grid&#39;]



        reg_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
        reg_sav_out_model = self.path_out + model + &#34;/model/&#34;



        try: #make new dir if needed
            os.makedirs(reg_sav_out_scores)
        except:
            None

        try: #make new dir if needed
            os.makedirs(reg_sav_out_model)
        except:
            None


        with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
            reg = LogGridSearch(reg_estimator, verbose = self.verbose, cv=self.cv, param_grid=reg_param_grid, scoring=&#34;neg_mean_absolute_error&#34;)
            reg_grid_search = reg.transformed_fit(self.X, self.y.values.ravel(), log)

        m2 = reg_grid_search.best_estimator_
        pickle.dump(m2, open(reg_sav_out_model  + self.species + &#39;_reg.sav&#39;, &#39;wb&#39;))

        print(&#34;exported model to: &#34; + reg_sav_out_model  + self.species + &#39;_reg.sav&#39;)

        with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
            reg_scores = cross_validate(m2, self.X, self.y.values.ravel(), cv = self.cv, verbose = self.verbose, scoring=reg_scoring)

        pickle.dump(reg_scores, open(reg_sav_out_scores + self.species + &#39;_reg.sav&#39;, &#39;wb&#39;))


        print(&#34;exported scoring to: &#34; + reg_sav_out_scores + self.species + &#39;_reg.sav&#39;)

        print(&#34;reg rRMSE: &#34; + str(int(round(np.mean(reg_scores[&#39;test_RMSE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
        print(&#34;reg rMAE: &#34; + str(int(round(np.mean(reg_scores[&#39;test_MAE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
        print(&#34;reg R2: &#34; + str(round(np.mean(reg_scores[&#39;test_R2&#39;]), 2)))



    if (classifier ==True) and (regressor ==True):
        print(&#34;training zero-inflated regressor&#34;)

        zir = ZeroInflatedRegressor(
            classifier=m1,
            regressor=m2,
        )

        zir_sav_out_scores = self.path_out + model + &#34;/scoring/&#34;
        zir_sav_out_model = self.path_out + model + &#34;/model/&#34;

        try: #make new dir if needed
            os.makedirs(zir_sav_out_scores)
        except:
            None

        try: #make new dir if needed
            os.makedirs(zir_sav_out_model)
        except:
            None           

        pickle.dump(zir, open(zir_sav_out_model + self.species + &#39;_zir.sav&#39;, &#39;wb&#39;))
        print(&#34;exported model to: &#34; + zir_sav_out_model + self.species + &#39;_zir.sav&#39;)


        with parallel_backend(&#39;multiprocessing&#39;, n_jobs=self.n_jobs):
            zir_scores = cross_validate(zir, self.X, self.y.ravel(), cv=self.cv, verbose =self.verbose, scoring=reg_scoring)


        pickle.dump(zir_scores, open(zir_sav_out_scores + self.species + &#39;_zir.sav&#39;, &#39;wb&#39;))
        print(&#34;exported scoring to: &#34; + zir_sav_out_scores + self.species + &#39;_zir.sav&#39;)

        print(&#34;zir rRMSE: &#34; + str(int(round(np.mean(zir_scores[&#39;test_RMSE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
        print(&#34;zir rMAE: &#34; + str(int(round(np.mean(zir_scores[&#39;test_MAE&#39;])/np.mean(self.y), 2)*-100))+&#34;%&#34;)
        print(&#34;zir R2: &#34; + str(round(np.mean(zir_scores[&#39;test_R2&#39;]), 2)))



    st = time.time()
    et = time.time()
    elapsed_time = et-st

    print(&#34;execution time:&#34;, elapsed_time, &#34;seconds&#34;)        </code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="planktonsdm" href="index.html">planktonsdm</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="planktonsdm.tune.tune" href="#planktonsdm.tune.tune">tune</a></code></h4>
<ul class="">
<li><code><a title="planktonsdm.tune.tune.train" href="#planktonsdm.tune.tune.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>