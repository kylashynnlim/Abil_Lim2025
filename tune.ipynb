{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "This is a notebook that shows you how to tune plankton ML models using the 'tune' class.\n",
    "\n",
    "This is the first notebook in a set of three:\n",
    "\n",
    "    - tune.ipynb: tune hyper-parameters to find the best model configuration\n",
    "\n",
    "    - predict.ipynb: make predictions using the best fitting model\n",
    "\n",
    "    - post.ipynb: analyse predictions and calculate metrics such as diversity\n",
    "\n",
    "There are several dependencies that need to be install prior to running this notebook:\n",
    "\n",
    "    pandas\n",
    "    numpy\n",
    "    scikit-learn\n",
    "    xgboost\n",
    "    joblib\n",
    "    \n",
    "\n",
    "Tuned models and scoring are saved using the following directory structure:\n",
    "\n",
    "    \n",
    "    /your_base_path/scoring/xgb/sppA_reg.sav\n",
    "    /your_base_path/scoring/rf/sppA_reg.sav\n",
    "    /your_base_path/scoring/rf/sppA_reg.sav\n",
    "\n",
    "    \n",
    "    /your_base_path/tuning/xgb/sppA_reg.sav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tune import tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setting up the model framework.\n",
    "\n",
    "Here we define the training data and model config within Python, but you could import your own training data \n",
    "from a .csv and load your model_config from a YAML.\n",
    "'''\n",
    "\n",
    "seed = 1 #random seed\n",
    "n_threads = 2 # how many cpu threads to use\n",
    "n_spp = 0 # which species to model\n",
    "path_out = \"/home/phyto/ModelOutput/test/\" #where to save model output\n",
    "\n",
    "d = pd.DataFrame({\"mld\":[50, 100, 120, 50, 50, 100, 120, 50, 200], \n",
    "                    \"temperature\":[25, 20, 15, 45, 25, 20, 15, 45, 10],\n",
    "                    \"Emiliania huxleyi\":[300000, 100000, 0, 6000, 9000, 5000, 3000, 0, 0],\n",
    "                    \"Coccolithus pelagicus\":[50000, 30000, 500, 800, 900, 0, 1000, 5000, 0]\n",
    "})\n",
    "\n",
    "model_config =  {\n",
    "\n",
    "    \"X_vars\" : [\"mld\", \"temperature\"],\n",
    "\n",
    "    \"species\" : [\"Emiliania huxleyi\", \"Coccolithus pelagicus\"],\n",
    "\n",
    "    \"reg_scoring\" : {\n",
    "                \"R2\":\"r2\",\n",
    "                \"MAE\": \"neg_mean_absolute_error\", \n",
    "                \"RMSE\":\"neg_root_mean_squared_error\",\n",
    "                },\n",
    "\n",
    "    \"clf_scoring\" : {\n",
    "                \"accuracy\": \"balanced_accuracy\",\n",
    "                },\n",
    "\n",
    "\n",
    "    \"rf_param_grid\":{\n",
    "\n",
    "        \"reg_param_grid\" : {\n",
    "                    'regressor__n_estimators': [100],\n",
    "                    'regressor__max_features': [5],\n",
    "                    'regressor__max_depth': [5],\n",
    "                    'regressor__min_samples_leaf': [0.5],\n",
    "                    'regressor__max_samples':[0.5]       \n",
    "                    },\n",
    "\n",
    "        \"clf_param_grid\" : {'n_estimators': [100],\n",
    "                    'max_features': [5],\n",
    "                    'max_depth': [5],\n",
    "                    'min_samples_leaf': [0.5],\n",
    "                    'max_samples': [0.5]\n",
    "                    }\n",
    "\n",
    "    },\n",
    "\n",
    "    \"xgb_param_grid\":{\n",
    "\n",
    "        \"clf_param_grid\" : {    \n",
    "                'eta':[0.01],       \n",
    "                'n_estimators': [10],\n",
    "                'max_depth': [4],\n",
    "                'subsample': [0.6],  \n",
    "                'colsample_bytree': [0.6],  \n",
    "                'gamma':[1],     \n",
    "                'alpha':[1]   \n",
    "                },     \n",
    "\n",
    "        \"reg_param_grid\" : {\n",
    "                        'regressor__eta': [0.01],\n",
    "                        'regressor__n_estimators': [10],\n",
    "                        'regressor__max_depth': [4],\n",
    "                        'regressor__colsample_bytree': [0.6],               \n",
    "                        'regressor__subsample': [0.6],          \n",
    "                        'regressor__gamma': [1],\n",
    "                        'regressor__alpha': [1]\n",
    "                        },\n",
    "\n",
    "    },\n",
    "\n",
    "    \"knn_param_grid\":{\n",
    "\n",
    "        \"clf_param_grid\" : {'max_samples': [0.5],\n",
    "                        'max_features': [0.5],\n",
    "                        'estimator__leaf_size': [30],\n",
    "                        'estimator__n_neighbors': [3],\n",
    "                        'estimator__p':  [1],           \n",
    "                        'estimator__weights': [\"uniform\"],                                       \n",
    "                        },\n",
    "\n",
    "\n",
    "        \"reg_param_grid\" : {'regressor__max_samples': [0.5],\n",
    "                        'regressor__max_features': [0.5],\n",
    "                        'regressor__estimator__leaf_size': [30],\n",
    "                        'regressor__estimator__n_neighbors': [3],\n",
    "                        'regressor__estimator__p':  [1],\n",
    "                        'regressor__estimator__weights': [\"uniform\"],                           \n",
    "                        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "X_vars = model_config[\"X_vars\"] \n",
    "species = model_config[\"species\"][n_spp]\n",
    "cv = 3\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.3654907602127317\n",
      "zir rMAE: -1.3654907602127317\n",
      "zir R2: -6.913892048232192\n",
      "execution time: 2.217594623565674 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1-phase Random forest \n",
    "'''\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "reg_param_grid = model_config['rf_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid, cv=cv, model=\"rf\", zir=False, log=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.366898625553064\n",
      "zir rMAE: -1.366898625553064\n",
      "zir R2: -7.2245129036588205\n",
      "execution time: 5.151210069656372 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2-phase Random forest \n",
    "note: for the 2-phase model we need to define the model configuration for both the classifier and the regressor\n",
    "'''\n",
    "\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "clf_scoring = model_config['clf_scoring']\n",
    "\n",
    "clf_param_grid = model_config['rf_param_grid']['clf_param_grid']\n",
    "reg_param_grid = model_config['rf_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid, clf_scoring = clf_scoring, clf_param_grid = clf_param_grid, \n",
    "      cv=cv, model=\"rf\", zir=True, log=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.3654907602127317\n",
      "zir rMAE: -1.3654907602127317\n",
      "zir R2: -6.913892048232192\n",
      "execution time: 3.7091381549835205 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing the impact of log transformation on the 1-phase Random forest \n",
    "\n",
    "note: we test both log and no-log by defining log=\"both\"\n",
    "'''\n",
    "\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "reg_param_grid = model_config['rf_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid, cv=cv, model=\"rf\", zir=False, log=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.3654907602127317\n",
      "zir rMAE: -1.3654907602127317\n",
      "zir R2: -6.913892048232192\n",
      "execution time: 2.3106985092163086 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1-phase Gradient boosting with XGBoost:\n",
    "'''\n",
    "\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "reg_param_grid = model_config['rf_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid, cv=cv, model=\"rf\", zir=False, log=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.3559173585491615\n",
      "zir rMAE: -1.3559173585491615\n",
      "zir R2: -5.67547992661585\n",
      "execution time: 1.121462106704712 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2-phase Gradient boosting with XGBoost:\n",
    "'''\n",
    "\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "clf_scoring = model_config['clf_scoring']\n",
    "\n",
    "clf_param_grid = model_config['xgb_param_grid']['clf_param_grid']\n",
    "reg_param_grid = model_config['xgb_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid, clf_scoring = clf_scoring, clf_param_grid = clf_param_grid,\n",
    "      cv=cv, model=\"xgb\", zir=True, log=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.3920747906582163\n",
      "zir rMAE: -1.3920747906582163\n",
      "zir R2: -13.9488704843248\n",
      "execution time: 1.3500545024871826 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1-phase nearest neighbors with a bagged KNN\n",
    "note: we need to define the number of bags when running KNN by defining bagging_estimators=30\n",
    "'''\n",
    "\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "reg_param_grid = model_config['knn_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid, cv=cv, model=\"knn\", zir=False, log=\"yes\", bagging_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tuning model\n",
      "zir rRMSE: -1.3594291344985059\n",
      "zir rMAE: -1.3594291344985059\n",
      "zir R2: -6.184075971103664\n",
      "execution time: 3.033029556274414 seconds\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2-phase nearest neighbors with a bagged KNN\n",
    "note: we need to define the number of bags when running KNN by defining bagging_estimators=30\n",
    "'''\n",
    "\n",
    "reg_scoring = model_config['reg_scoring']\n",
    "clf_scoring = model_config['clf_scoring']\n",
    "\n",
    "clf_param_grid = model_config['knn_param_grid']['clf_param_grid']\n",
    "reg_param_grid = model_config['knn_param_grid']['reg_param_grid']\n",
    "\n",
    "m = tune(d, X_vars, species, seed, n_threads, verbose, cv, path_out, hot_encode=False)\n",
    "m.XGB(reg_scoring, reg_param_grid,  clf_scoring = clf_scoring, clf_param_grid = clf_param_grid,  \n",
    "      cv=cv, model=\"knn\", zir=True, log=\"yes\", bagging_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO:\n",
    "    \n",
    "Add print statement for log=\"both\"\n",
    "\n",
    "Add tau scoring\n",
    "\n",
    "Add one_hot_encoding\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
